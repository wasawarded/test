   #alternate Edit this page Wikipedia (en) Wikipedia Atom feed

   Jump to content

   [ ] Main menu
   Main menu
   (BUTTON) move to sidebar (BUTTON) hide
   Navigation
     * Main page
     * Contents
     * Current events
     * Random article
     * About Wikipedia
     * Contact us

   Contribute
     * Help
     * Learn to edit
     * Community portal
     * Recent changes
     * Upload file

   Wikipedia The Free Encyclopedia
   Search
   ____________________
   (BUTTON) Search

   [ ] Appearance

     * Donate
     * Create account
     * Log in

   [ ] Personal tools
     * Donate
     * Create account
     * Log in

   Pages for logged out editors learn more
     * Contributions
     * Talk

Contents

   (BUTTON) move to sidebar (BUTTON) hide
     * (Top)
     * 1 History
     * 2 Concepts
       (BUTTON) Toggle Concepts subsection
          + 2.1 Algorithm = Logic + Control
          + 2.2 Relationship with functional programming
          + 2.3 Relationship with relational programming
          + 2.4 Semantics of Horn clause programs
          + 2.5 Negation as failure
          + 2.6 Metalogic programming
          + 2.7 Relationship with the Computational-representational
            understanding of mind
          + 2.8 Knowledge representation
     * 3 Variants and extensions
       (BUTTON) Toggle Variants and extensions subsection
          + 3.1 Prolog
          + 3.2 Constraint logic programming
          + 3.3 Datalog
          + 3.4 Answer set programming
          + 3.5 Abductive logic programming
          + 3.6 Inductive logic programming
          + 3.7 Concurrent logic programming
          + 3.8 Concurrent constraint logic programming
          + 3.9 Higher-order logic programming
          + 3.10 Linear logic programming
          + 3.11 Object-oriented logic programming
          + 3.12 Transaction logic programming
     * 4 See also
     * 5 Citations
     * 6 Sources
       (BUTTON) Toggle Sources subsection
          + 6.1 General introductions
          + 6.2 Other sources
     * 7 Further reading
     * 8 External links

   [ ] Toggle the table of contents

Logic programming

   [ ] 36 languages
     * العربية
     * বাংলা
     * Български
     * Bosanski
     * Català
     * Čeština
     * Deutsch
     * Eesti
     * Español
     * فارسی
     * Français
     * Gaeilge
     * Galego
     * 한국어
     * Hrvatski
     * Ido
     * Italiano
     * עברית
     * Bahasa Melayu
     * Nederlands
     * 日本語
     * Polski
     * Português
     * Русиньскый
     * Русский
     * Shqip
     * Simple English
     * Српски / srpski
     * Suomi
     * Svenska
     * ไทย
     * Türkçe
     * Українська
     * Tiếng Việt
     * 粵語
     * 中文

   Edit links

     * Article
     * Talk

   [ ] English

     * Read
     * Edit
     * View history

   [ ] Tools
   Tools
   (BUTTON) move to sidebar (BUTTON) hide
   Actions
     * Read
     * Edit
     * View history

   General
     * What links here
     * Related changes
     * Upload file
     * Special pages
     * Permanent link
     * Page information
     * Cite this page
     * Get shortened URL
     * Download QR code

   Print/export
     * Download as PDF
     * Printable version

   In other projects
     * Wikimedia Commons
     * Wikidata item

   Appearance
   (BUTTON) move to sidebar (BUTTON) hide
   From Wikipedia, the free encyclopedia
   Programming paradigm based on formal logic

   Logic programming is a programming, database and knowledge
   representation paradigm based on formal logic. A logic program is a set
   of sentences in logical form, representing knowledge about some problem
   domain. Computation is performed by applying logical reasoning to that
   knowledge, to solve problems in the domain. Major logic programming
   language families include Prolog, Answer Set Programming (ASP) and
   Datalog. In all of these languages, rules are written in the form of
   clauses:

          A :- B[1], ..., B[n].

   and are read as declarative sentences in logical form:

          A if B[1] and ... and B[n].

   A is called the head of the rule, B[1], ..., B[n] is called the body,
   and the B[i] are called literals or conditions. When n = 0, the rule is
   called a fact and is written in the simplified form:

          A.

   Queries (or goals) have the same syntax as the bodies of rules and are
   commonly written in the form:

          ?- B[1], ..., B[n].

   In the simplest case of Horn clauses (or "definite" clauses), all of
   the A, B[1], ..., B[n] are atomic formulae of the form p(t[1] ,...,
   t[m]), where p is a predicate symbol naming a relation, like
   "motherhood", and the t[i] are terms naming objects (or individuals).
   Terms include both constant symbols, like "charles", and variables,
   such as X, which start with an upper case letter.

   Consider, for example, the following Horn clause program:
mother_child(elizabeth, charles).
father_child(charles, william).
father_child(charles, harry).
parent_child(X, Y) :-
     mother_child(X, Y).
parent_child(X, Y) :-
     father_child(X, Y).
grandparent_child(X, Y) :-
     parent_child(X, Z),
     parent_child(Z, Y).

   Given a query, the program produces answers. For instance for a query
   ?- parent_child(X, william), the single answer is
X = charles

   Various queries can be asked. For instance the program can be queried
   both to generate grandparents and to generate grandchildren. It can
   even be used to generate all pairs of grandchildren and grandparents,
   or simply to check if a given pair is such a pair:
grandparent_child(X, william).
X = elizabeth

?- grandparent_child(elizabeth, Y).
Y = william;
Y = harry.

?- grandparent_child(X, Y).
X = elizabeth
Y = william;
X = elizabeth
Y = harry.

?- grandparent_child(william, harry).
no
?- grandparent_child(elizabeth, harry).
yes

   Although Horn clause logic programs are Turing complete,^[1]^[2] for
   most practical applications, Horn clause programs need to be extended
   to "normal" logic programs with negative conditions. For example, the
   definition of sibling uses a negative condition, where the predicate =
   is defined by the clause X = X :
sibling(X, Y) :-
     parent_child(Z, X),
     parent_child(Z, Y),
     not(X = Y).

   Logic programming languages that include negative conditions have the
   knowledge representation capabilities of a non-monotonic logic.

   In ASP and Datalog, logic programs have only a declarative reading, and
   their execution is performed by means of a proof procedure or model
   generator whose behaviour is not meant to be controlled by the
   programmer. However, in the Prolog family of languages, logic programs
   also have a procedural interpretation as goal-reduction procedures.
   From this point of view, clause A :- B[1],...,B[n] is understood as:

          to solve A, solve B[1], and ... and solve B[n].

   Negative conditions in the bodies of clauses also have a procedural
   interpretation, known as negation as failure: A negative literal not B
   is deemed to hold if and only if the positive literal B fails to hold.

   Much of the research in the field of logic programming has been
   concerned with trying to develop a logical semantics for negation as
   failure and with developing other semantics and other implementations
   for negation. These developments have been important, in turn, for
   supporting the development of formal methods for logic-based program
   verification and program transformation.

History

   [edit]

   The use of mathematical logic to represent and execute computer
   programs is also a feature of the lambda calculus, developed by Alonzo
   Church in the 1930s. However, the first proposal to use the clausal
   form of logic for representing computer programs was made by Cordell
   Green.^[3] This used an axiomatization of a subset of LISP, together
   with a representation of an input-output relation, to compute the
   relation by simulating the execution of the program in LISP. Foster and
   Elcock's Absys, on the other hand, employed a combination of equations
   and lambda calculus in an assertional programming language that places
   no constraints on the order in which operations are performed.^[4]

   Logic programming, with its current syntax of facts and rules, can be
   traced back to debates in the late 1960s and early 1970s about
   declarative versus procedural representations of knowledge in
   artificial intelligence. Advocates of declarative representations were
   notably working at Stanford, associated with John McCarthy, Bertram
   Raphael and Cordell Green, and in Edinburgh, with John Alan Robinson
   (an academic visitor from Syracuse University), Pat Hayes, and Robert
   Kowalski. Advocates of procedural representations were mainly centered
   at MIT, under the leadership of Marvin Minsky and Seymour Papert.^[5]

   Although it was based on the proof methods of logic, Planner, developed
   by Carl Hewitt at MIT, was the first language to emerge within this
   proceduralist paradigm.^[6] Planner featured pattern-directed
   invocation of procedural plans from goals (i.e. goal-reduction or
   backward chaining) and from assertions (i.e. forward chaining). The
   most influential implementation of Planner was the subset of Planner,
   called Micro-Planner, implemented by Gerry Sussman, Eugene Charniak and
   Terry Winograd. Winograd used Micro-Planner to implement the landmark,
   natural-language understanding program SHRDLU.^[7] For the sake of
   efficiency, Planner used a backtracking control structure so that only
   one possible computation path had to be stored at a time. Planner gave
   rise to the programming languages QA4,^[8] Popler,^[9] Conniver,^[10]
   QLISP,^[11] and the concurrent language Ether.^[12]

   Hayes and Kowalski in Edinburgh tried to reconcile the logic-based
   declarative approach to knowledge representation with Planner's
   procedural approach. Hayes (1973) developed an equational language,
   Golux, in which different procedures could be obtained by altering the
   behavior of the theorem prover.^[13]

   In the meanwhile, Alain Colmerauer in Marseille was working on
   natural-language understanding, using logic to represent semantics and
   using resolution for question-answering. During the summer of 1971,
   Colmerauer invited Kowalski to Marseille, and together they discovered
   that the clausal form of logic could be used to represent formal
   grammars and that resolution theorem provers could be used for parsing.
   They observed that some theorem provers, like hyper-resolution,^[14]
   behave as bottom-up parsers and others, like SL resolution (1971)^[15]
   behave as top-down parsers.

   It was in the following summer of 1972, that Kowalski, again working
   with Colmerauer, developed the procedural interpretation of
   implications in clausal form. It also became clear that such clauses
   could be restricted to definite clauses or Horn clauses, and that
   SL-resolution could be restricted (and generalised) to SLD resolution.
   Kowalski's procedural interpretation and SLD were described in a 1973
   memo, published in 1974.^[16]

   Colmerauer, with Philippe Roussel, used the procedural interpretation
   as the basis of Prolog, which was implemented in the summer and autumn
   of 1972. The first Prolog program, also written in 1972 and implemented
   in Marseille, was a French question-answering system. The use of Prolog
   as a practical programming language was given great momentum by the
   development of a compiler by David H. D. Warren in Edinburgh in 1977.
   Experiments demonstrated that Edinburgh Prolog could compete with the
   processing speed of other symbolic programming languages such as
   Lisp.^[17] Edinburgh Prolog became the de facto standard and strongly
   influenced the definition of ISO standard Prolog.

   Logic programming gained international attention during the 1980s, when
   it was chosen by the Japanese Ministry of International Trade and
   Industry to develop the software for the Fifth Generation Computer
   Systems (FGCS) project. The FGCS project aimed to use logic programming
   to develop advanced Artificial Intelligence applications on massively
   parallel computers. Although the project initially explored the use of
   Prolog, it later adopted the use of concurrent logic programming,
   because it was closer to the FGCS computer architecture.

   However, the committed choice feature of concurrent logic programming
   interfered with the language's logical semantics^[18] and with its
   suitability for knowledge representation and problem solving
   applications. Moreover, the parallel computer systems developed in the
   project failed to compete with advances taking place in the development
   of more conventional, general-purpose computers. Together these two
   issues resulted in the FGCS project failing to meet its objectives.
   Interest in both logic programming and AI fell into world-wide
   decline.^[19]

   In the meanwhile, more declarative logic programming approaches,
   including those based on the use of Prolog, continued to make progress
   independently of the FGCS project. In particular, although Prolog was
   developed to combine declarative and procedural representations of
   knowledge, the purely declarative interpretation of logic programs
   became the focus for applications in the field of deductive databases.
   Work in this field became prominent around 1977, when Hervé Gallaire
   and Jack Minker organized a workshop on logic and databases in
   Toulouse.^[20] The field was eventually renamed as Datalog.

   This focus on the logical, declarative reading of logic programs was
   given further impetus by the development of constraint logic
   programming in the 1980s and Answer Set Programming in the 1990s. It is
   also receiving renewed emphasis in recent applications of Prolog^[21]

   The Association for Logic Programming (ALP) was founded in 1986 to
   promote Logic Programming. Its official journal until 2000, was The
   Journal of Logic Programming. Its founding editor-in-chief was J. Alan
   Robinson.^[22] In 2001, the journal was renamed The Journal of Logic
   and Algebraic Programming, and the official journal of ALP became
   Theory and Practice of Logic Programming, published by Cambridge
   University Press.

Concepts

   [edit]

   Logic programs enjoy a rich variety of semantics and problem solving
   methods, as well as a wide range of applications in programming,
   databases, knowledge representation and problem solving.

Algorithm = Logic + Control

   [edit]

   The procedural interpretation of logic programs, which uses backward
   reasoning to reduce goals to subgoals, is a special case of the use of
   a problem-solving strategy to control the use of a declarative, logical
   representation of knowledge to obtain the behaviour of an algorithm.
   More generally, different problem-solving strategies can be applied to
   the same logical representation to obtain different algorithms.
   Alternatively, different algorithms can be obtained with a given
   problem-solving strategy by using different logical
   representations.^[23]

   The two main problem-solving strategies are backward reasoning (goal
   reduction) and forward reasoning, also known as top-down and bottom-up
   reasoning, respectively.

   In the simple case of a propositional Horn clause program and a
   top-level atomic goal, backward reasoning determines an and-or tree,
   which constitutes the search space for solving the goal. The top-level
   goal is the root of the tree. Given any node in the tree and any clause
   whose head matches the node, there exists a set of child nodes
   corresponding to the sub-goals in the body of the clause. These child
   nodes are grouped together by an "and". The alternative sets of
   children corresponding to alternative ways of solving the node are
   grouped together by an "or".

   Any search strategy can be used to search this space. Prolog uses a
   sequential, last-in-first-out, backtracking strategy, in which only one
   alternative and one sub-goal are considered at a time. For example,
   subgoals can be solved in parallel, and clauses can also be tried in
   parallel. The first strategy is called and-parallel
   and the second strategy is called or-parallel. Other search strategies,
   such as intelligent backtracking,^[24] or best-first search to find an
   optimal solution,^[25] are also possible.

   In the more general, non-propositional case, where sub-goals can share
   variables, other strategies can be used, such as choosing the subgoal
   that is most highly instantiated or that is sufficiently instantiated
   so that only one procedure applies.^[26] Such strategies are used, for
   example, in concurrent logic programming.

   In most cases, backward reasoning from a query or goal is more
   efficient than forward reasoning. But sometimes with Datalog and Answer
   Set Programming, there may be no query that is separate from the set of
   clauses as a whole, and then generating all the facts that can be
   derived from the clauses is a sensible problem-solving strategy. Here
   is another example, where forward reasoning beats backward reasoning in
   a more conventional computation task, where the goal ?- fibonacci(n,
   Result) is to find the n^th fibonacci number:
fibonacci(0, 0).
fibonacci(1, 1).

fibonacci(N, Result) :-
    N > 1,
    N1 is N - 1,
    N2 is N - 2,
    fibonacci(N1, F1),
    fibonacci(N2, F2),
    Result is F1 + F2.

   Here the relation fibonacci(N, M) stands for the function fibonacci(N)
   = M, and the predicate N is Expression is Prolog notation for the
   predicate that instantiates the variable N to the value of Expression.

   Given the goal of computing the fibonacci number of n, backward
   reasoning reduces the goal to the two subgoals of computing the
   fibonacci numbers of n-1 and n-2. It reduces the subgoal of computing
   the fibonacci number of n-1 to the two subgoals of computing the
   fibonacci numbers of n-2 and n-3, redundantly computing the fibonacci
   number of n-2. This process of reducing one fibonacci subgoal to two
   fibonacci subgoals continues until it reaches the numbers 0 and 1. Its
   complexity is of the order 2^n. In contrast, forward reasoning
   generates the sequence of fibonacci numbers, starting from 0 and 1
   without any recomputation, and its complexity is linear with respect to
   n.

   Prolog cannot perform forward reasoning directly. But it can achieve
   the effect of forward reasoning within the context of backward
   reasoning by means of tabling: Subgoals are maintained in a table,
   along with their solutions. If a subgoal is re-encountered, it is
   solved directly by using the solutions already in the table, instead of
   re-solving the subgoals redundantly.^[27]

Relationship with functional programming

   [edit]
   See also: Functional programming § Comparison to logic programming

   Logic programming can be viewed as a generalisation of functional
   programming, in which functions are a special case of relations.^[28]
   For example, the function, mother(X) = Y, (every X has only one mother
   Y) can be represented by the relation mother(X, Y). In this respect,
   logic programs are similar to relational databases, which also
   represent functions as relations.

   Compared with relational syntax, functional syntax is more compact for
   nested functions. For example, in functional syntax the definition of
   maternal grandmother can be written in the nested form:
maternal_grandmother(X) = mother(mother(X)).

   The same definition in relational notation needs to be written in the
   unnested, flattened form:
maternal_grandmother(X, Y) :- mother(X, Z), mother(Z, Y).

   However, nested syntax can be regarded as syntactic sugar for unnested
   syntax. Ciao Prolog, for example, transforms functional syntax into
   relational form and executes the resulting logic program using the
   standard Prolog execution strategy.^[29] Moreover, the same
   transformation can be used to execute nested relations that are not
   functional. For example:
grandparent(X) := parent(parent(X)).
parent(X) := mother(X).
parent(X) := father(X).

mother(charles) := elizabeth.
father(charles) := phillip.
mother(harry) := diana.
father(harry) := charles.

?- grandparent(X,Y).
X = harry,
Y = elizabeth.
X = harry,
Y = phillip.

Relationship with relational programming

   [edit]

   The term relational programming has been used to cover a variety of
   programming languages that treat functions as a special case of
   relations. Some of these languages, such as miniKanren^[28] and
   relational linear programming^[30] are logic programming languages in
   the sense of this article.

   However, the relational language RML is an imperative programming
   language ^[31] whose core construct is a relational expression, which
   is similar to an expression in first-order predicate logic.

   Other relational programming languages are based on the relational
   calculus^[32] or relational algebra.^[33]

Semantics of Horn clause programs

   [edit]
   Main article: Syntax and semantics of logic programming

   Viewed in purely logical terms, there are two approaches to the
   declarative semantics of Horn clause logic programs: One approach is
   the original logical consequence semantics, which understands solving a
   goal as showing that the goal is a theorem that is true in all models
   of the program.

   In this approach, computation is theorem-proving in first-order logic;
   and both backward reasoning, as in SLD resolution, and forward
   reasoning, as in hyper-resolution, are correct and complete
   theorem-proving methods. Sometimes such theorem-proving methods are
   also regarded as providing a separate proof-theoretic (or operational)
   semantics for logic programs. But from a logical point of view, they
   are proof methods, rather than semantics.

   The other approach to the declarative semantics of Horn clause programs
   is the satisfiability semantics, which understands solving a goal as
   showing that the goal is true (or satisfied) in some intended (or
   standard) model of the program. For Horn clause programs, there always
   exists such a standard model: It is the unique minimal model of the
   program.

   Informally speaking, a minimal model is a model that, when it is viewed
   as the set of all (variable-free) facts that are true in the model,
   contains no smaller set of facts that is also a model of the program.

   For example, the following facts represent the minimal model of the
   family relationships example in the introduction of this article. All
   other variable-free facts are false in the model:
mother_child(elizabeth, charles).
father_child(charles, william).
father_child(charles, harry).
parent_child(elizabeth, charles).
parent_child(charles, william).
parent_child(charles, harry).
grandparent_child(elizabeth, william).
grandparent_child(elizabeth, harry).

   The satisfiability semantics also has an alternative, more mathematical
   characterisation as the least fixed point of the function that uses the
   rules in the program to derive new facts from existing facts in one
   step of inference.

   Remarkably, the same problem-solving methods of forward and backward
   reasoning, which were originally developed for the logical consequence
   semantics, are equally applicable to the satisfiability semantics:
   Forward reasoning generates the minimal model of a Horn clause program,
   by deriving new facts from existing facts, until no new additional
   facts can be generated. Backward reasoning, which succeeds by reducing
   a goal to subgoals, until all subgoals are solved by facts, ensures
   that the goal is true in the minimal model, without generating the
   model explicitly.^[34]

   The difference between the two declarative semantics can be seen with
   the definitions of addition and multiplication in successor arithmetic,
   which represents the natural numbers 0, 1, 2, ... as a sequence of
   terms of the form 0, s(0), s(s(0)), .... In general, the term s(X)
   represents the successor of X, namely X + 1. Here are the standard
   definitions of addition and multiplication in functional notation:
     X + 0 = X.
     X + s(Y)    = s(X + Y).
i.e. X + (Y + 1) = (X + Y) + 1

     X × 0 = 0.
     X × s(Y)    = X + (X × Y).
i.e. X × (Y + 1) = X + (X × Y).

   Here are the same definitions as a logic program, using add(X, Y, Z) to
   represent X + Y = Z, and multiply(X, Y, Z) to represent X × Y = Z:
add(X, 0, X).
add(X, s(Y), s(Z)) :- add(X, Y, Z).

multiply(X, 0, 0).
multiply(X, s(Y), W) :- multiply(X, Y, Z), add(X, Z, W).

   The two declarative semantics both give the same answers for the same
   existentially quantified conjunctions of addition and multiplication
   goals. For example 2 × 2 = X has the solution X = 4; and X × X = X + X
   has two solutions X = 0 and X = 2:
?- multiply(s(s(0)), s(s(0)), X).
X = s(s(s(s(0)))).

?- multiply(X, X, Y), add(X, X, Y).
X = 0, Y = 0.
X = s(s(0)), Y = s(s(s(s(0)))).

   However, with the logical-consequence semantics, there are non-standard
   models of the program, in which, for example, add(s(s(0)), s(s(0)),
   s(s(s(s(s(0)))))), i.e. 2 + 2 = 5 is true. But with the satisfiability
   semantics, there is only one model, namely the standard model of
   arithmetic, in which 2 + 2 = 5 is false.

   In both semantics, the goal ?- add(s(s(0)), s(s(0)), s(s(s(s(s(0))))))
   fails. In the satisfiability semantics, the failure of the goal means
   that the truth value of the goal is false. But in the logical
   consequence semantics, the failure means that the truth value of the
   goal is unknown.

Negation as failure

   [edit]
   Main article: Negation as failure

   Negation as failure (NAF), as a way of concluding that a negative
   condition not p holds by showing that the positive condition p fails to
   hold, was already a feature of early Prolog systems. The resulting
   extension of SLD resolution is called SLDNF. A similar construct,
   called "thnot", also existed in Micro-Planner.

   The logical semantics of NAF was unresolved until Keith Clark^[35]
   showed that, under certain natural conditions, NAF is an efficient,
   correct (and sometimes complete) way of reasoning with the logical
   consequence semantics using the completion of a logic program in
   first-order logic.

   Completion amounts roughly to regarding the set of all the program
   clauses with the same predicate in the head, say:

          A :- Body[1].
          ...
          A :- Body[k].

   as a definition of the predicate:

          A iff (Body[1] or ... or Body[k])

   where iff means "if and only if". The completion also includes axioms
   of equality, which correspond to unification. Clark showed that proofs
   generated by SLDNF are structurally similar to proofs generated by a
   natural deduction style of reasoning with the completion of the
   program.

   Consider, for example, the following program:
should_receive_sanction(X, punishment) :-
    is_a_thief(X),
    not should_receive_sanction(X, rehabilitation).

should_receive_sanction(X, rehabilitation) :-
    is_a_thief(X),
    is_a_minor(X),
    not is_violent(X).

is_a_thief(tom).

   Given the goal of determining whether tom should receive a sanction,
   the first rule succeeds in showing that tom should be punished:
?- should_receive_sanction(tom, Sanction).
Sanction = punishment.

   This is because tom is a thief, and it cannot be shown that tom should
   be rehabilitated. It cannot be shown that tom should be rehabilitated,
   because it cannot be shown that tom is a minor.

   If, however, we receive new information that tom is indeed a minor, the
   previous conclusion that tom should be punished is replaced by the new
   conclusion that tom should be rehabilitated:
minor(tom).

?- should_receive_sanction(tom, Sanction).
Sanction = rehabilitation.

   This property of withdrawing a conclusion when new information is
   added, is called non-monotonicity, and it makes logic programming a
   non-monotonic logic.

   But, if we are now told that tom is violent, the conclusion that tom
   should be punished will be reinstated:
violent(tom).

?- should_receive_sanction(tom, Sanction).
Sanction = punishment.

   The completion of this program is:
should_receive_sanction(X, Sanction) iff
    Sanction = punishment, is_a_thief(X),
    not should_receive_sanction(X, rehabilitation)
 or Sanction = rehabilitation, is_a_thief(X), is_a_minor(X),
    not is_violent(X).

is_a_thief(X) iff X = tom.
is_a_minor(X) iff X = tom.
is_violent(X) iff X = tom.

   The notion of completion is closely related to John McCarthy's
   circumscription semantics for default reasoning,^[36] and to Ray
   Reiter's closed world assumption.^[37]

   The completion semantics for negation is a logical consequence
   semantics, for which SLDNF provides a proof-theoretic implementation.
   However, in the 1980s, the satisfiability semantics became more popular
   for logic programs with negation. In the satisfiability semantics,
   negation is interpreted according to the classical definition of truth
   in an intended or standard model of the logic program.

   In the case of logic programs with negative conditions, there are two
   main variants of the satisfiability semantics: In the well-founded
   semantics, the intended model of a logic program is a unique,
   three-valued, minimal model, which always exists. The well-founded
   semantics generalises the notion of inductive definition in
   mathematical logic.^[38] XSB Prolog^[39] implements the well-founded
   semantics using SLG resolution.^[40]

   In the alternative stable model semantics, there may be no intended
   models or several intended models, all of which are minimal and
   two-valued. The stable model semantics underpins answer set programming
   (ASP).

   Both the well-founded and stable model semantics apply to arbitrary
   logic programs with negation. However, both semantics coincide for
   stratified logic programs. For example, the program for sanctioning
   thieves is (locally) stratified, and all three semantics for the
   program determine the same intended model:
should_receive_sanction(tom, punishment).
is_a_thief(tom).
is_a_minor(tom).
is_violent(tom).

   Attempts to understand negation in logic programming have also
   contributed to the development of abstract argumentation
   frameworks.^[41] In an argumentation interpretation of negation, the
   initial argument that tom should be punished because he is a thief, is
   attacked by the argument that he should be rehabilitated because he is
   a minor. But the fact that tom is violent undermines the argument that
   tom should be rehabilitated and reinstates the argument that tom should
   be punished.

Metalogic programming

   [edit]

   Metaprogramming, in which programs are treated as data, was already a
   feature of early Prolog implementations.^[42]^[43] For example, the
   Edinburgh DEC10 implementation of Prolog included "an interpreter and a
   compiler, both written in Prolog itself".^[43] The simplest metaprogram
   is the so-called "vanilla" meta-interpreter:
    solve(true).
    solve((B,C)):- solve(B),solve(C).
    solve(A):- clause(A,B),solve(B).

   where true represents an empty conjunction, and (B,C) is a composite
   term representing the conjunction of B and C. The predicate clause(A,B)
   means that there is a clause of the form A :- B.

   Metaprogramming is an application of the more general use of a
   metalogic or metalanguage to describe and reason about another
   language, called the object language.

   Metalogic programming allows object-level and metalevel representations
   to be combined, as in natural language. For example, in the following
   program, the atomic formula attends(Person, Meeting) occurs both as an
   object-level formula, and as an argument of the metapredicates
   prohibited and approved.
prohibited(attends(Person, Meeting)) :-
    not(approved(attends(Person, Meeting))).

should_receive_sanction(Person, scolding) :- attends(Person, Meeting),
    lofty(Person), prohibited(attends(Person, Meeting)).
should_receive_sanction(Person, banishment) :- attends(Person, Meeting),
    lowly(Person), prohibited(attends(Person, Meeting)).

approved(attends(alice, tea_party)).
attends(mad_hatter, tea_party).
attends(dormouse, tea_party).

lofty(mad_hatter).
lowly(dormouse).

?- should_receive_sanction(X,Y).
Person = mad_hatter,
Sanction = scolding.
Person = dormouse,
Sanction = banishment.

Relationship with the Computational-representational understanding of mind

   [edit]

   In his popular Introduction to Cognitive Science,^[44] Paul Thagard
   includes logic and rules as alternative approaches to modelling human
   thinking. He argues that rules, which have the form IF condition THEN
   action, are "very similar" to logical conditionals, but they are
   simpler and have greater psychological plausability (page 51). Among
   other differences between logic and rules, he argues that logic uses
   deduction, but rules use search (page 45) and can be used to reason
   either forward or backward (page 47). Sentences in logic "have to be
   interpreted as universally true", but rules can be defaults, which
   admit exceptions (page 44).

   He states that "unlike logic, rule-based systems can also easily
   represent strategic information about what to do" (page 45). For
   example, "IF you want to go home for the weekend, and you have bus
   fare, THEN you can catch a bus". He does not observe that the same
   strategy of reducing a goal to subgoals can be interpreted, in the
   manner of logic programming, as applying backward reasoning to a
   logical conditional:
can_go(you, home) :- have(you, bus_fare), catch(you, bus).

   All of these characteristics of rule-based systems - search, forward
   and backward reasoning, default reasoning, and goal-reduction - are
   also defining characteristics of logic programming. This suggests that
   Thagard's conclusion (page 56) that:

     Much of human knowledge is naturally described in terms of rules,
     and many kinds of thinking such as planning can be modeled by
     rule-based systems.

   also applies to logic programming.

   Other arguments showing how logic programming can be used to model
   aspects of human thinking are presented by Keith Stenning and Michiel
   van Lambalgen in their book, Human Reasoning and Cognitive
   Science.^[45] They show how the non-monotonic character of logic
   programs can be used to explain human performance on a variety of
   psychological tasks. They also show (page 237) that "closed–world
   reasoning in its guise as logic programming has an appealing neural
   implementation, unlike classical logic."

   In The Proper Treatment of Events,^[46] Michiel van Lambalgen and Fritz
   Hamm investigate the use of constraint logic programming to code
   "temporal notions in natural language by looking at the way human
   beings construct time".

Knowledge representation

   [edit]

   The use of logic to represent procedural knowledge and strategic
   information was one of the main goals contributing to the early
   development of logic programming. Moreover, it continues to be an
   important feature of the Prolog family of logic programming languages
   today. However, many applications of logic programming, including
   Prolog applications, increasingly focus on the use of logic to
   represent purely declarative knowledge. These applications include both
   the representation of general commonsense knowledge and the
   representation of domain specific expertise.

   Commonsense includes knowledge about cause and effect, as formalised,
   for example, in the situation calculus, event calculus and action
   languages. Here is a simplified example, which illustrates the main
   features of such formalisms. The first clause states that a fact holds
   immediately after an event initiates (or causes) the fact. The second
   clause is a frame axiom, which states that a fact that holds at a time
   continues to hold at the next time unless it is terminated by an event
   that happens at the time. This formulation allows more than one event
   to occur at the same time:
holds(Fact, Time2) :-
    happens(Event, Time1),
    Time2 is Time1 + 1,
    initiates(Event, Fact).

holds(Fact, Time2) :-
        happens(Event, Time1),
    Time2 is Time1 + 1,
    holds(Fact, Time1),
    not(terminated(Fact, Time1)).

terminated(Fact, Time) :-
   happens(Event, Time),
   terminates(Event, Fact).

   Here holds is a meta-predicate, similar to solve above. However,
   whereas solve has only one argument, which applies to general clauses,
   the first argument of holds is a fact and the second argument is a time
   (or state). The atomic formula holds(Fact, Time) expresses that the
   Fact holds at the Time. Such time-varying facts are also called
   fluents. The atomic formula happens(Event, Time) expresses that the
   Event happens at the Time.

   The following example illustrates how these clauses can be used to
   reason about causality in a toy blocks world. Here, in the initial
   state at time 0, a green block is on a table and a red block is stacked
   on the green block (like a traffic light). At time 0, the red block is
   moved to the table. At time 1, the green block is moved onto the red
   block. Moving an object onto a place terminates the fact that the
   object is on any place, and initiates the fact that the object is on
   the place to which it is moved:
holds(on(green_block, table), 0).
holds(on(red_block, green_block), 0).

happens(move(red_block, table), 0).
happens(move(green_block, red_block), 1).

initiates(move(Object, Place), on(Object, Place)).
terminates(move(Object, Place2), on(Object, Place1)).

?- holds(Fact, Time).

Fact = on(green_block,table),
Time = 0.
Fact = on(red_block,green_block),
Time = 0.
Fact = on(green_block,table),
Time = 1.
Fact = on(red_block,table),
Time = 1.
Fact = on(green_block,red_block),
Time = 2.
Fact = on(red_block,table),
Time = 2.

   Forward reasoning and backward reasoning generate the same answers to
   the goal holds(Fact, Time). But forward reasoning generates fluents
   progressively in temporal order, and backward reasoning generates
   fluents regressively, as in the domain-specific use of regression in
   the situation calculus.^[47]

   Logic programming has also proved to be useful for representing
   domain-specific expertise in expert systems.^[48] But human expertise,
   like general-purpose commonsense, is mostly implicit and tacit, and it
   is often difficult to represent such implicit knowledge in explicit
   rules. This difficulty does not arise, however, when logic programs are
   used to represent the existing, explicit rules of a business
   organisation or legal authority.

   For example, here is a representation of a simplified version of the
   first sentence of the British Nationality Act, which states that a
   person who is born in the UK becomes a British citizen at the time of
   birth if a parent of the person is a British citizen at the time of
   birth:
initiates(birth(Person), citizen(Person, uk)):-
    time_of(birth(Person), Time),
    place_of(birth(Person), uk),
    parent_child(Another_Person, Person),
    holds(citizen(Another_Person, uk), Time).

   Historically, the representation of a large portion of the British
   Nationality Act as a logic program in the 1980s^[49] was "hugely
   influential for the development of computational representations of
   legislation, showing how logic programming enables intuitively
   appealing representations that can be directly deployed to generate
   automatic inferences".^[50]

   More recently, the PROLEG system,^[51] initiated in 2009 and consisting
   of approximately 2500 rules and exceptions of civil code and supreme
   court case rules in Japan, has become possibly the largest legal rule
   base in the world.^[52]

Variants and extensions

   [edit]

Prolog

   [edit]
   Main article: Prolog

   The SLD resolution rule of inference is neutral about the order in
   which subgoals in the bodies of clauses can be selected for solution.
   For the sake of efficiency, Prolog restricts this order to the order in
   which the subgoals are written. SLD is also neutral about the strategy
   for searching the space of SLD proofs. Prolog searches this space,
   top-down, depth-first, trying different clauses for solving the same
   (sub)goal in the order in which the clauses are written.

   This search strategy has the advantage that the current branch of the
   tree can be represented efficiently by a stack. When a goal clause at
   the top of the stack is reduced to a new goal clause, the new goal
   clause is pushed onto the top of the stack. When the selected subgoal
   in the goal clause at the top of the stack cannot be solved, the search
   strategy backtracks, removing the goal clause from the top of the
   stack, and retrying the attempted solution of the selected subgoal in
   the previous goal clause using the next clause that matches the
   selected subgoal.

   Backtracking can be restricted by using a subgoal, called cut, written
   as !, which always succeeds but cannot be backtracked. Cut can be used
   to improve efficiency, but can also interfere with the logical meaning
   of clauses. In many cases, the use of cut can be replaced by negation
   as failure. In fact, negation as failure can be defined in Prolog, by
   using cut, together with any literal, say fail, that unifies with the
   head of no clause:
not(P) :- P, !, fail.
not(P).

   Prolog provides other features, in addition to cut, that do not have a
   logical interpretation. These include the built-in predicates assert
   and retract for destructively updating the state of the program during
   program execution.

   For example, the toy blocks world example above can be implemented
   without frame axioms using destructive change of state:
on(green_block, table).
on(red_block, green_block).

move(Object, Place2) :-
        retract(on(Object, Place1)),
        assert(on(Object, Place2).

   The sequence of move events and the resulting locations of the blocks
   can be computed by executing the query:
?- move(red_block, table), move(green_block, red_block), on(Object, Place).

Object = red_block,
Place = table.
Object = green_block,
Place = red_block.

   Various extensions of logic programming have been developed to provide
   a logical framework for such destructive change of
   state.^[53]^[54]^[55]

   The broad range of Prolog applications, both in isolation and in
   combination with other languages is highlighted in the Year of Prolog
   Book,^[21] celebrating the 50 year anniversary of Prolog in 2022.

   Prolog has also contributed to the development of other programming
   languages, including ALF, Fril, Gödel, Mercury, Oz, Ciao, Visual
   Prolog, XSB, and λProlog.

Constraint logic programming

   [edit]
   Main article: Constraint logic programming

   Constraint logic programming (CLP) combines Horn clause logic
   programming with constraint solving. It extends Horn clauses by
   allowing some predicates, declared as constraint predicates, to occur
   as literals in the body of a clause. Constraint predicates are not
   defined by the facts and rules in the program, but are predefined by
   some domain-specific model-theoretic structure or theory.

   Procedurally, subgoals whose predicates are defined by the program are
   solved by goal-reduction, as in ordinary logic programming, but
   constraints are simplified and checked for satisfiability by a
   domain-specific constraint-solver, which implements the semantics of
   the constraint predicates. An initial problem is solved by reducing it
   to a satisfiable conjunction of constraints.

   Interestingly, the first version of Prolog already included a
   constraint predicate dif(term1, term2), from Philippe Roussel's 1972
   PhD thesis, which succeeds if both of its arguments are different
   terms, but which is delayed if either of the terms contains a
   variable.^[52]

   The following constraint logic program represents a toy temporal
   database of john's history as a teacher:
teaches(john, hardware, T) :- 1990 ≤ T, T < 1999.
teaches(john, software, T) :- 1999 ≤ T, T < 2005.
teaches(john, logic, T) :- 2005 ≤ T, T ≤ 2012.
rank(john, instructor, T) :- 1990 ≤ T, T < 2010.
rank(john, professor, T) :- 2010 ≤ T, T < 2014.

   Here ≤ and < are constraint predicates, with their usual intended
   semantics. The following goal clause queries the database to find out
   when john both taught logic and was a professor:
?- teaches(john, logic, T), rank(john, professor, T).

   The solution 2010 ≤ T, T ≤ 2012 results from simplifying the
   constraints 2005 ≤ T, T ≤ 2012, 2010 ≤ T, T < 2014.

   Constraint logic programming has been used to solve problems in such
   fields as civil engineering, mechanical engineering, digital circuit
   verification, automated timetabling, air traffic control, and finance.
   It is closely related to abductive logic programming.

Datalog

   [edit]
   Main article: Datalog

   Datalog is a database definition language, which combines a relational
   view of data, as in relational databases, with a logical view, as in
   logic programming.

   Relational databases use a relational calculus or relational algebra,
   with relational operations, such as union, intersection, set difference
   and cartesian product to specify queries, which access a database.
   Datalog uses logical connectives, such as or, and and not in the bodies
   of rules to define relations as part of the database itself.

   It was recognized early in the development of relational databases that
   recursive queries cannot be expressed in either relational algebra or
   relational calculus, and that this defficiency can be remedied by
   introducing a least-fixed-point operator.^[56]^[57] In contrast,
   recursive relations can be defined naturally by rules in logic
   programs, without the need for any new logical connectives or
   operators.

   Datalog differs from more general logic programming by having only
   constants and variables as terms. Moreover, all facts are
   variable-free, and rules are restricted, so that if they are executed
   bottom-up, then the derived facts are also variable-free.

   For example, consider the family database:
mother_child(elizabeth, charles).
father_child(charles, william).
father_child(charles, harry).
parent_child(X, Y) :-
     mother_child(X, Y).
parent_child(X, Y) :-
     father_child(X, Y).
ancestor_descendant(X, Y) :-
     parent_child(X, X).
ancestor_descendant(X, Y) :-
     ancestor_descendant(X, Z),
     ancestor_descendant(Z, Y).

   Bottom-up execution derives the following set of additional facts and
   terminates:
parent_child(elizabeth, charles).
parent_child(charles, william).
parent_child(charles, harry).

ancestor_descendant(elizabeth, charles).
ancestor_descendant(charles, william).
ancestor_descendant(charles, harry).

ancestor_descendant(elizabeth, william).
ancestor_descendant(elizabeth, harry).

   Top-down execution derives the same answers to the query:
?- ancestor_descendant(X, Y).

   But then it goes into an infinite loop. However, top-down execution
   with tabling gives the same answers and terminates without looping.

Answer set programming

   [edit]

   Main article: Answer Set Programming

   Like Datalog, Answer Set programming (ASP) is not Turing-complete.
   Moreover, instead of separating goals (or queries) from the program to
   be used in solving the goals, ASP treats the whole program as a goal,
   and solves the goal by generating a stable model that makes the goal
   true. For this purpose, it uses the stable model semantics, according
   to which a logic program can have zero, one or more intended models.
   For example, the following program represents a degenerate variant of
   the map colouring problem of colouring two countries red or green:

country(oz).
country(iz).
adjacent(oz, iz).
colour(C, red) :- country(C), not(colour(C, green)).
colour(C, green) :- country(C), not(colour(C, red)).

   The problem has four solutions represented by four stable models:

country(oz). country(iz). adjacent(oz, iz). colour(oz, red).   colour(iz, red).

country(oz). country(iz). adjacent(oz, iz). colour(oz, green). colour(iz, green)
.

country(oz). country(iz). adjacent(oz, iz). colour(oz, red).   colour(iz, green)
.

country(oz). country(iz). adjacent(oz, iz). colour(oz, green). colour(iz, red).

   To represent the standard version of the map colouring problem, we need
   to add a constraint that two adjacent countries cannot be coloured the
   same colour. In ASP, this constraint can be written as a clause of the
   form:

:- country(C1), country(C2), adjacent(C1, C2), colour(C1, X), colour(C2, X).

   With the addition of this constraint, the problem now has only two
   solutions:

country(oz). country(iz). adjacent(oz, iz). colour(oz, red).   colour(iz, green)
.

country(oz). country(iz). adjacent(oz, iz). colour(oz, green). colour(iz, red).

   The addition of constraints of the form :- Body. eliminates models in
   which Body is true.

   Confusingly, constraints in ASP are different from constraints in CLP.
   Constraints in CLP are predicates that qualify answers to queries (and
   solutions of goals). Constraints in ASP are clauses that eliminate
   models that would otherwise satisfy goals. Constraints in ASP are like
   integrity constraints in databases.

   This combination of ordinary logic programming clauses and constraint
   clauses illustrates the generate-and-test methodology of problem
   solving in ASP: The ordinary clauses define a search space of possible
   solutions, and the constraints filter out unwanted solutions.^[58]

   Most implementations of ASP proceed in two steps: First they
   instantiate the program in all possible ways, reducing it to a
   propositional logic program (known as grounding). Then they apply a
   propositional logic problem solver, such as the DPLL algorithm or a
   Boolean SAT solver. However, some implementations, such as s(CASP)^[59]
   use a goal-directed, top-down, SLD resolution-like procedure without
   grounding.

Abductive logic programming

   [edit]

   Main article: Abductive logic programming

   Abductive logic programming^[60] (ALP), like CLP, extends normal logic
   programming by allowing the bodies of clauses to contain literals whose
   predicates are not defined by clauses. In ALP, these predicates are
   declared as abducible (or assumable), and are used as in abductive
   reasoning to explain observations, or more generally to add new facts
   to the program (as assumptions) to solve goals.

   For example, suppose we are given an initial state in which a red block
   is on a green block on a table at time 0:

holds(on(green_block, table), 0).
holds(on(red_block, green_block), 0).

   Suppose we are also given the goal:

?- holds(on(green_block,red_block), 3), holds(on(red_block,table), 3).

   The goal can represent an observation, in which case a solution is an
   explanation of the observation. Or the goal can represent a desired
   future state of affairs, in which case a solution is a plan for
   achieving the goal.^[61]

   We can use the rules for cause and effect presented earlier to solve
   the goal, by treating the happens predicate as abducible:

holds(Fact, Time2) :-
    happens(Event, Time1),
    Time2 is Time1 + 1,
    initiates(Event, Fact).

holds(Fact, Time2) :-
        happens(Event, Time1),
    Time2 is Time1 + 1,
    holds(Fact, Time1),
    not(terminated(Fact, Time1)).

terminated(Fact, Time) :-
   happens(Event, Time),
   terminates(Event, Fact).

initiates(move(Object, Place), on(Object, Place)).
terminates(move(Object, Place2), on(Object, Place1)).

   ALP solves the goal by reasoning backwards and adding assumptions to
   the program, to solve abducible subgoals. In this case there are many
   alternative solutions, including:

happens(move(red_block, table), 0).
happens(tick, 1).
happens(move(green_block, red_block), 2).

happens(tick,0).
happens(move(red_block, table), 1).
happens(move(green_block, red_block), 2).

happens(move(red_block, table), 0).
happens(move(green_block, red_block), 1).
happens(tick, 2).

   Here tick is an event that marks the passage of time without initiating
   or terminating any fluents.

   There are also solutions in which the two move events happen at the
   same time. For example:

happens(move(red_block, table), 0).
happens(move(green_block, red_block), 0).
happens(tick, 1).
happens(tick, 2).

   Such solutions, if not desired, can be removed by adding an integrity
   constraint, which is like a constraint clause in ASP:

:- happens(move(Block1, Place), Time), happens(move(Block2, Block1), Time).

   Abductive logic programming has been used for fault diagnosis,
   planning, natural language processing and machine learning. It has also
   been used to interpret negation as failure as a form of abductive
   reasoning.^[62]

Inductive logic programming

   [edit]

   Main article: Inductive logic programming

   Inductive logic programming (ILP) is an approach to machine learning
   that induces logic programs as hypothetical generalisations of positive
   and negative examples. Given a logic program representing background
   knowledge and positive examples together with constraints representing
   negative examples, an ILP system induces a logic program that
   generalises the positive examples while excluding the negative
   examples.

   ILP is similar to ALP, in that both can be viewed as generating
   hypotheses to explain observations, and as employing constraints to
   exclude undesirable hypotheses. But in ALP the hypotheses are
   variable-free facts, and in ILP the hypotheses are general
   rules.^[63]^[64]

   For example, given only background knowledge of the mother_child and
   father_child relations, and suitable examples of the grandparent_child
   relation, current ILP systems can generate the definition of
   grandparent_child, inventing an auxiliary predicate, which can be
   interpreted as the parent_child relation:^[65]

grandparent_child(X, Y):- auxiliary(X, Z), auxiliary(Z, Y).
auxiliary(X, Y):- mother_child(X, Y).
auxiliary(X, Y):- father_child(X, Y).

   Stuart Russell^[66] has referred to such invention of new concepts as
   the most important step needed for reaching human-level AI.

   Recent work in ILP, combining logic programming, learning and
   probability, has given rise to the fields of statistical relational
   learning and probabilistic inductive logic programming.

Concurrent logic programming

   [edit]

   Main article: Concurrent logic programming

   Concurrent logic programming integrates concepts of logic programming
   with concurrent programming. Its development was given a big impetus in
   the 1980s by its choice for the systems programming language of the
   Japanese Fifth Generation Project (FGCS).^[67]

   A concurrent logic program is a set of guarded Horn clauses of the
   form:

                H :- G[1], ..., G[n] | B[1], ..., B[n].

   The conjunction G[1], ... , G[n] is called the guard of the clause, and
   | is the commitment operator. Declaratively, guarded Horn clauses are
   read as ordinary logical implications:

                H if G[1] and ... and G[n] and B[1] and ... and B[n].

   However, procedurally, when there are several clauses whose heads H
   match a given goal, then all of the clauses are executed in parallel,
   checking whether their guards G[1], ... , G[n] hold. If the guards of
   more than one clause hold, then a committed choice is made to one of
   the clauses, and execution proceeds with the subgoals B[1], ..., B[n]
   of the chosen clause. These subgoals can also be executed in parallel.
   Thus concurrent logic programming implements a form of "don't care
   nondeterminism", rather than "don't know nondeterminism".

   For example, the following concurrent logic program defines a predicate
   shuffle(Left, Right, Merge), which can be used to shuffle two lists
   Left and Right, combining them into a single list Merge that preserves
   the ordering of the two lists Left and Right:

shuffle([], [], []).
shuffle(Left, Right, Merge) :-
    Left = [First | Rest] |
    Merge = [First | ShortMerge],
    shuffle(Rest, Right, ShortMerge).
shuffle(Left, Right, Merge) :-
    Right = [First | Rest] |
    Merge = [First | ShortMerge],
    shuffle(Left, Rest, ShortMerge).

   Here, [] represents the empty list, and [Head | Tail] represents a list
   with first element Head followed by list Tail, as in Prolog. (Notice
   that the first occurrence of | in the second and third clauses is the
   list constructor, whereas the second occurrence of | is the commitment
   operator.) The program can be used, for example, to shuffle the lists
   [ace, queen, king] and [1, 4, 2] by invoking the goal clause:

shuffle([ace, queen, king], [1, 4, 2], Merge).

   The program will non-deterministically generate a single solution, for
   example Merge = [ace, queen, 1, king, 4, 2].

   Carl Hewitt has argued^[68] that, because of the indeterminacy of
   concurrent computation, concurrent logic programming cannot implement
   general concurrency. However, according to the logical semantics, any
   result of a computation of a concurrent logic program is a logical
   consequence of the program, even though not all logical consequences
   can be derived.

Concurrent constraint logic programming

   [edit]

   Main article: Concurrent constraint logic programming

   Concurrent constraint logic programming^[69] combines concurrent logic
   programming and constraint logic programming, using constraints to
   control concurrency. A clause can contain a guard, which is a set of
   constraints that may block the applicability of the clause. When the
   guards of several clauses are satisfied, concurrent constraint logic
   programming makes a committed choice to use only one.

Higher-order logic programming

   [edit]

   Several researchers have extended logic programming with higher-order
   programming features derived from higher-order logic, such as predicate
   variables. Such languages include the Prolog extensions HiLog^[70] and
   λProlog.^[71]

Linear logic programming

   [edit]

   Basing logic programming within linear logic has resulted in the design
   of logic programming languages that are considerably more expressive
   than those based on classical logic. Horn clause programs can only
   represent state change by the change in arguments to predicates. In
   linear logic programming, one can use the ambient linear logic to
   support state change. Some early designs of logic programming languages
   based on linear logic include LO,^[72] Lolli,^[73] ACL,^[74] and
   Forum.^[75] Forum provides a goal-directed interpretation of all linear
   logic.

Object-oriented logic programming

   [edit]

   F-logic^[76] extends logic programming with objects and the frame
   syntax.

   Logtalk^[77] extends the Prolog programming language with support for
   objects, protocols, and other OOP concepts. It supports most
   standard-compliant Prolog systems as backend compilers.

Transaction logic programming

   [edit]

   Transaction logic^[53] is an extension of logic programming with a
   logical theory of state-modifying updates. It has both a
   model-theoretic semantics and a procedural one. An implementation of a
   subset of Transaction logic is available in the Flora-2^[78] system.
   Other prototypes are also available.

See also

   [edit]

     * Automated theorem proving
     * Boolean satisfiability problem
     * Constraint logic programming
     * Control theory
     * Datalog
     * Fril
     * Functional programming
     * Fuzzy logic
     * Inductive logic programming
     * Linear logic
     * Logic in computer science (includes Formal methods)
     * Logic programming languages
     * Programmable logic controller
     * R++
     * Reasoning system
     * Rule-based machine learning
     * Satisfiability
     * Syntax and semantics of logic programming

Citations

   [edit]

    1. ^ Tärnlund, S.Å. (1977). "Horn clause computability". BIT Numerical
       Mathematics. 17 (2): 215–226. doi:10.1007/BF01932293.
       S2CID 32577496.
    2. ^ Andréka, H.; Németi, I. (1978). "The generalised completeness of
       Horn predicate-logic as a programming language". Acta Cybernetica.
       4 (1): 3–10.
    3. ^ Green, Cordell. Application of Theorem Proving to Problem Solving
       (PDF). IJCAI 1969.
    4. ^ Foster, J.M.; Elcock, E.W. (1969). ABSYS 1: An Incremental
       Compiler for Assertions: an Introduction. Fourth Annual Machine
       Intelligence Workshop. Machine Intelligence. Vol. 4. Edinburgh, UK:
       Edinburgh University Press. pp. 423–429.
    5. ^ Kowalski, R. A. (1988). "The early years of logic programming"
       (PDF). Communications of the ACM. 31: 38–43.
       doi:10.1145/35043.35046. S2CID 12259230.
    6. ^ Hewitt, Carl. Planner: A Language for Proving Theorems in Robots
       (PDF). IJCAI 1969.
    7. ^ Winograd, Terry (1972). "Understanding natural language".
       Cognitive Psychology. 3 (1): 1–191.
       doi:10.1016/0010-0285(72)90002-3.
    8. ^ Jeff Rulifson; Jan Derksen; Richard Waldinger (November 1973).
       QA4, A Procedural Calculus for Intuitive Reasoning (PDF) (Technical
       report). SRI AI Center Technical Note 73.
    9. ^ Davies, J.M., 1971. POPLER: a POP-2 planner. Edinburgh
       University, Department of Machine Intelligence and Perception.
   10. ^ McDermott, D.V.; Sussman, G.J. (May 1972). The Conniver reference
       manual (Technical report). Artificial Intelligence Memo No. 259.
   11. ^ Reboh, R.; Sacerdoti, E.D. (August 1973). A preliminary QLISP
       manual (Technical report). Artificial Intelligence Center, SRI
       International.
   12. ^ Kornfeld, W.A.; Hewitt, C.E. (1981). "The scientific community
       metaphor". IEEE Transactions on Systems, Man, and Cybernetics. 11
       (1): 24–33. doi:10.1109/TSMC.1981.4308575. hdl:1721.1/5693.
       S2CID 1322857.
   13. ^ Hayes, Pat (1973). "Computation and Deduction". Proceedings of
       the 2nd MFCS Symposium. Czechoslovak Academy of Sciences.
       pp. 105–118.
   14. ^ Robinson, J. (1965). "Automatic deduction with hyper-resolution".
       International Journal of Computer Mathematics. 1 (3): 227–234.
       doi:10.2307/2272384. JSTOR 2272384.
   15. ^ Kowalski, Robert; Kuehner, Donald (1971). "Linear Resolution with
       Selection Function" (PDF). Artificial Intelligence. 2 (3–4):
       227–260. doi:10.1016/0004-3702(71)90012-9.
   16. ^ Kowalski, Robert (1973). "Predicate Logic as a Programming
       Language" (PDF). Department of Artificial Intelligence, Edinburgh
       University. Memo 70. Also in Proceedings IFIP Congress, Stockholm,
       North Holland Publishing Co., 1974, pp. 569–574.
   17. ^ Warren, D.H.; Pereira, L.M.; Pereira, F. (1977). "Prolog-the
       language and its implementation compared with Lisp". ACM SIGPLAN
       Notices. 12 (8): 109–115. doi:10.1145/872734.806939.
   18. ^ Ueda, K., 2018. Logic/constraint programming and concurrency: The
       hard-won lessons of the fifth generation computer project. Science
       of Computer Programming, 164, pp.3-17.
   19. ^ H.P. Newquist, 2020. The Brain Makers: The History Of Artificial
       Intelligence. The Relayer Group.
   20. ^ Gallaire, Hervé; Minker, John 'Jack', eds. (1978), "Logic and
       Data Bases, Symposium on Logic and Data Bases, Centre d'études et
       de recherches de Toulouse, 1977", Advances in Data Base Theory, New
       York: Plenum Press, ISBN 978-0-306-40060-5.
   21. ^ ^a ^b Warren, D.S. (2023). "Introduction to Prolog". In Warren,
       D.S.; Dahl, V.; Eiter, T.; Hermenegildo, M.V.; Kowalski, R.; Rossi,
       F. (eds.). Prolog: The Next 50 Years. Lecture Notes in Computer
       Science(). Vol. 13900. Springer, Cham. pp. 3–19.
       doi:10.1007/978-3-031-35254-6_1. ISBN 978-3-031-35253-9.
   22. ^ Robinson, J. Alan (2001). "Invited Editorial". Theory and
       Practice of Logic Programming. 1 (1). Cambridge University Press:
       1. doi:10.1017/s1471068400000028 (inactive 1 November 2024).{{cite
       journal}}: CS1 maint: DOI inactive as of November 2024 (link)
   23. ^ R.A.Kowalski (July 1979). "Algorithm=Logic + Control".
       Communications of the ACM. 22 (7): 424–436.
       doi:10.1145/359131.359136. S2CID 2509896.
   24. ^ Bruynooghe, M.; Pereira, L.M. (1984). "Deduction revision by
       intelligent backtracking". Implementations of Prolog. Chichester,
       England: Ellis Horwood. pp. 194–215.
   25. ^ Nakamura, K. (July 1985). Heuristic Prolog: logic program
       execution by heuristic search. Conference on Logic Programming.
       Berlin, Heidelberg: Springer Berlin Heidelberg. pp. 148–155.
   26. ^ Genesereth, M.R.; Ginsberg, M.L. (1985). "Logic programming".
       Communications of the ACM. 28 (9): 933–941. doi:10.1145/4284.4287.
       S2CID 15527861.
   27. ^ Swift, T.; Warren, D.S. (January 2012). "XSB: Extending Prolog
       with tabled logic programming". Theory and Practice of Logic
       Programming. 12 (1–2): 157–187. arXiv:1012.5123.
       doi:10.1017/S1471068411000500. S2CID 6153112.
   28. ^ ^a ^b Daniel Friedman; William Byrd; Oleg Kiselyov; Jason Hemann
       (2018). The Reasoned Schemer, Second Edition. The MIT Press.
   29. ^ A. Casas, D. Cabeza, M. V. Hermenegildo. A Syntactic Approach to
       Combining Functional Notation, Lazy Evaluation and Higher-Order in
       LP Systems. The 8th International Symposium on Functional and Logic
       Programming (FLOPS'06), pages 142-162, April 2006.
   30. ^ Kersting, K., Mladenov, M. and Tokmakov, P., 2017. Relational
       linear programming. Artificial Intelligence, 244, pp.188-216.
   31. ^ Beyer, D., 2006, May. Relational programming with CrocoPat. In
       Proceedings of the 28th International Conference on Software
       engineering (pp. 807-810).
   32. ^ MacLennan, B.J., 1983. Overview of relational programming. ACM
       SIGPLAN Notices, 18(3), pp.36-45.
   33. ^ Behnke, R., Berghammer, R., Meyer, E. and Schneider, P., 1998.
       RELVIEW—A system for calculating with relations and relational
       programming. In Fundamental Approaches to Software Engineering:
       First International Conference, FASE'98 Held as Part of the Joint
       European Conferences on Theory and Practice of Software, ETAPS'98
       Lisbon, Portugal, March 28–April 4, 1998 Proceedings 1 (pp.
       318-321). Springer Berlin Heidelberg.
   34. ^ Van Emden, M.H.; Kowalski, R.A. (October 1976). "The semantics of
       predicate logic as a programming language". Journal of the ACM. 23
       (4): 733–742. doi:10.1145/321978.321991. S2CID 11048276.
   35. ^ Clark, K.L. (1977). "Negation as Failure". Logic and Data Bases.
       Boston, MA: Springer US. pp. 293–322.
       doi:10.1007/978-1-4684-3384-5_11. ISBN 978-1-4684-3386-9.
   36. ^ Gelfond, M.; Przymusinska, H.; Przymusinski, T. (1989). "On the
       relationship between circumscription and negation as failure".
       Artificial Intelligence. 38 (1): 75–94.
       doi:10.1016/0004-3702(89)90068-4.
   37. ^ Shepherdson, J.C. (1984). "Negation as failure: a comparison of
       Clark's completed data base and Reiter's closed world assumption".
       The Journal of Logic Programming. 1 (1): 51–79.
       doi:10.1016/0743-1066(84)90023-2.
   38. ^ Denecker, M.; Ternovska, E. (2008). "A logic of nonmonotone
       inductive definitions". ACM Transactions on Computational Logic. 9
       (2): 14:1–14:52. arXiv:cs/0501025. doi:10.1145/1342991.1342998.
       S2CID 13156469.
   39. ^ Rao, P.; Sagonas, K.; Swift, T.; Warren, D.S.; Freire, J. (July
       28–31, 1997). XSB: A system for efficiently computing well-founded
       semantics. Logic Programming And Nonmonotonic Reasoning: 4th
       International Conference, LPNMR'97. Dagstuhl Castle, Germany:
       Springer Berlin Heidelberg. pp. 430–440.
       doi:10.1007/3-540-63255-7_33.
   40. ^ W. Chen; D. S. Warren (January 1996). "Tabled Evaluation with
       Delaying for General Logic Programs". Journal of the ACM. 43 (1):
       20–74. doi:10.1145/227595.227597. S2CID 7041379.
   41. ^ Phan Minh Dung (1995). "On the acceptability of arguments and its
       fundamental role in nonmonotonic reasoning, logic programming, and
       n–person games". Artificial Intelligence. 77 (2): 321–357.
       doi:10.1016/0004-3702(94)00041-X.
   42. ^ Colmerauer, A. and Roussel, P., 1996. The birth of Prolog. In
       History of programming languages---II (pp. 331-367).
   43. ^ ^a ^b Warren, D.H., Pereira, L.M. and Pereira, F., 1977.
       Prolog-the language and its implementation compared with Lisp. ACM
       SIGPLAN Notices, 12(8), pp.109-115.
   44. ^ Thagard, Paul (2005). Mind: Introduction to Cognitive Science.
       The MIT Press. p. 11.
       ISBN 9780262701099.https://www.google.co.uk/books/edition/Mind_seco
       nd_edition/gjcR1U2HT7kC?hl=en&gbpv=1&pg=PP11&printsec=frontcover
   45. ^ Stenning, Keith; van Lambalgen, Michiel (2008). Human reasoning
       and cognitive science. MIT Press.
       ISBN 978-0-262-19583-6.https://philpapers.org/archive/STEHRA-5.pdf
   46. ^ Van Lambalgen, M. and Hamm, F., 2008. The proper treatment of
       events. John Wiley & Sons.
       https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=3126
       320bb6e37ca3727fed404828b53fc56ff063
   47. ^ Reiter, R., 1991. The frame problem in the situation calculus: A
       simple solution (sometimes) and a completeness result for goal
       regression. Artificial and Mathematical Theory of Computation, 3.
   48. ^ Merritt, D., 2012. Building expert systems in Prolog. Springer
       Science & Business Media.
       https://ds.amu.edu.et/xmlui/bitstream/handle/123456789/4434/%28Text
       %20Book%29%20Building%20Expert%20Systems%20in%20Prolog.pdf?sequence
       =1&isAllowed=y
   49. ^ Sergot, M.J.; Sadri, F.; Kowalski, R.A.; Kriwaczek, F.; Hammond,
       P; Cory, H.T. (1986). "The British Nationality Act as a logic
       program" (PDF). Communications of the ACM. 29 (5): 370–386.
       doi:10.1145/5689.5920. S2CID 5665107.
   50. ^ Prakken, H.; Sartor, G. (October 2015). "Law and logic: a review
       from an argumentation perspective" (PDF). Artificial Intelligence.
       227: 214–245. doi:10.1016/j.artint.2015.06.005. S2CID 4261497.
   51. ^ Satoh, K., 2023. PROLEG: Practical legal reasoning system. In
       Prolog: The Next 50 Years (pp. 277-283). Cham: Springer Nature
       Switzerland.
   52. ^ ^a ^b Körner, Philipp; Leuschel, Michael; Barbosa, João; Costa,
       Vítor Santos; Dahl, Verónica; Hermenegildo, Manuel V.; Morales,
       Jose F.; Wielemaker, Jan; Diaz, Daniel; Abreu, Salvador; Ciatto,
       Giovanni (November 2022). "Fifty Years of Prolog and Beyond".
       Theory and Practice of Logic Programming. 22 (6): 776–858.
       arXiv:2201.10816. doi:10.1017/S1471068422000102. ISSN 1471-0684.
   53. ^ ^a ^b Bonner, A.J. and Kifer, M., 1993, February. Transaction
       Logic Programming. In ICLP (Vol. 93, pp. 257-279).
   54. ^ Genesereth, M., 2023. Dynamic logic programming. In Prolog: The
       Next 50 Years (pp. 197-209). Cham: Springer Nature Switzerland.
   55. ^ Kowalski, R., Sadri, F., Calejo, M. and Dávila, J., 2023.
       Combining logic programming and imperative programming in LPS. In
       Prolog: The Next 50 Years (pp. 210-223). Cham: Springer Nature
       Switzerland.
   56. ^ Aho, A.V. and Ullman, J.D., 1979, January. Universality of data
       retrieval languages. In Proceedings of the 6th ACM SIGACT-SIGPLAN
       symposium on Principles of programming languages (pp. 110-119).
   57. ^ Maier, D., Tekle, K.T., Kifer, M. and Warren, D.S., 2018.
       Datalog: concepts, history, and outlook. In Declarative Logic
       Programming: Theory, Systems, and Applications (pp. 3-100).
   58. ^ Eiter, T., Ianni, G. and Krennwallner, T., 2009. Answer Set
       Programming: A Primer. In Reasoning Web. Semantic Technologies for
       Information Systems: 5th International Summer School 2009,
       Brixen-Bressanone, Italy, August 30-September 4, 2009, Tutorial
       Lectures (pp. 40-110).
   59. ^ Arias, J.; Carro, M.; Salazar, E.; Marple, K.; Gupta, G. (2018).
       "Constraint Answer Set Programming without Grounding". Theory and
       Practice of Logic Programming. 18 (3–4): 337–354. arXiv:1804.11162.
       doi:10.1017/S1471068418000285. S2CID 13754645.
   60. ^ Denecker, M.; Kakas, A.C. (July 2000). "Special issue: abductive
       logic programming". Journal of Logic Programming. 44 (1–3): 1–4.
       doi:10.1016/S0743-1066(99)00078-3.
   61. ^ Eshghi, K., 1988, August. Abductive Planning with Event Calculus.
       In ICLP/SLP (pp. 562-579).
   62. ^ Eshghi, K. and Kowalski, R.A., 1989, June. Abduction Compared
       with Negation by Failure. In ICLP (Vol. 89, pp. 234-255).
   63. ^ Nienhuys-Cheng, Shan-hwei; Wolf, Ronald de (1997). Foundations of
       inductive logic programming. Lecture notes in computer science
       Lecture notes in artificial intelligence. Berlin Heidelberg:
       Springer. p. 173. ISBN 978-3-540-62927-6.
   64. ^ Flach, P.A. and Kakas, A.C., 2000. On the relation between
       abduction and inductive learning. In Abductive Reasoning and
       Learning (pp. 1-33). Dordrecht: Springer Netherlands.
   65. ^ Cropper, A. and Dumančić, S., 2022. Inductive logic programming
       at 30: a new introduction. Journal of Artificial Intelligence
       Research, 74, pp.765-850.
   66. ^ Russell, S., 2019. Human compatible: Artificial intelligence and
       the problem of control. Penguin.
   67. ^ Shunichi Uchida and Kazuhiro Fuchi. Proceedings of the FGCS
       Project Evaluation Workshop. Institute for New Generation Computer
       Technology (ICOT). 1992.
   68. ^ Hewitt, Carl (27 April 2016). "Inconsistency Robustness for Logic
       Programs". Hal Archives. pp. 21–26. Retrieved 7 November 2016.
   69. ^ Saraswat, V.A. and Rinard, M., 1989, December. Concurrent
       constraint programming. In Proceedings of the 17th ACM
       SIGPLAN-SIGACT symposium on Principles of programming languages
       (pp. 232-245).
   70. ^ Chen, Weidong; Kifer, Michael; Warren, David S. (February 1993).
       "HiLog: A foundation for higher-order logic programming". Journal
       of Logic Programming. 15 (3): 187–230.
       doi:10.1016/0743-1066(93)90039-J.
   71. ^ Miller, D.A. and Nadathur, G., 1986, July. Higher-order logic
       programming. In International Conference on Logic Programming (pp.
       448-462). Berlin, Heidelberg: Springer Berlin Heidelberg.
   72. ^ Andreoli, Jean-Marc (1 June 1992). "Logic Programming with
       Focusing Proofs in Linear Logic". Journal of Logic and Computation.
       2 (3): 297–347. doi:10.1093/logcom/2.3.297.
   73. ^ Hodas, Joshua; Miller, Dale (1994). "Logic Programming in a
       Fragment of Intuitionistic Linear Logic". Information and
       Computation. 110 (2): 327–365. doi:10.1006/inco.1994.1036.
   74. ^ Kobayashi, Naoki; Yonezawa, Akinori (1994). Asynchronous
       communication model based on linear logic. US/Japan Workshop on
       Parallel Symbolic Computing. pp. 279–294. CiteSeerX 10.1.1.42.8749.
   75. ^ Miller, Dale (30 September 1996). "Forum: A Multiple-Conclusion
       Specification Logic". Theoretical Computer Science. 165 (1):
       201–232. doi:10.1016/0304-3975(96)00045-X.
   76. ^ Kifer, M. and Lausen, G., 1989, June. F-logic: a higher-order
       language for reasoning about objects, inheritance, and scheme. In
       Proceedings of the 1989 ACM SIGMOD international conference on
       Management of data (pp. 134-146).
   77. ^ de Moura, P.J.L., 2003. Design of an Object-Oriented Logic
       Programming Language (Doctoral dissertation, Universidade da Beira
       Interior).
   78. ^ Yang, G. and Kifer, M., 2000, July. FLORA: Implementing an
       efficient DOOD system using a tabling logic engine. In
       International Conference on Computational Logic (pp. 1078-1093).
       Berlin, Heidelberg: Springer Berlin Heidelberg.

Sources

   [edit]

General introductions

   [edit]

     *

   Baral, C.; Gelfond, M. (1994). "Logic programming and knowledge
   representation" (PDF). The Journal of Logic Programming. 19–20: 73–148.
   doi:10.1016/0743-1066(94)90025-6.

     Kowalski, R. A. (1988). "The early years of logic programming" (PDF).
   Communications of the ACM. 31: 38–43. doi:10.1145/35043.35046.
   S2CID 12259230. [1]

     Lloyd, J. W. (1987). Foundations of Logic Programming (2nd ed.).
   Springer-Verlag.

Other sources

   [edit]

     * John McCarthy. "Programs with common sense". Symposium on
       Mechanization of Thought Processes. National Physical Laboratory.
       Teddington, England. 1958.
     *

   Miller, Dale; Nadathur, Gopalan; Pfenning, Frank; Scedrov, Andre
   (1991). "Uniform proofs as a foundation for logic programming". Annals
   of Pure and Applied Logic. 51 (1–2): 125–157.
   doi:10.1016/0168-0072(91)90068-W.

     Ehud Shapiro (Editor). Concurrent Prolog. MIT Press. 1987.

     James Slagle. "Experiments with a Deductive Question-Answering
   Program". CACM. December 1965.

     Gabbay, Dov M.; Hogger, Christopher John; Robinson, J.A., eds.
   (1993-1998). Handbook of Logic in Artificial Intelligence and Logic
   Programming.Vols. 1–5, Oxford University Press.

Further reading

   [edit]

     * Carl Hewitt. "Procedural Embedding of Knowledge in Planner". IJCAI
       1971.
     * Carl Hewitt. "The Repeated Demise of Logic Programming and Why It
       Will Be Reincarnated". AAAI Spring Symposium: What Went Wrong and
       Why: Lessons from AI Research and Applications 2006: 2–9.
     * Evgeny Dantsin, Thomas Eiter, Georg Gottlob, Andrei Voronkov:
       Complexity and expressive power of logic programming. ACM Comput.
       Surv. 33(3): 374–425 (2001)
     * Ulf Nilsson and Jan Maluszynski, Logic, Programming and Prolog

External links

   [edit]

   Wikimedia Commons has media related to Logic programming.

     * Logic Programming Virtual Library entry
     * Bibliographies on Logic Programming Archived 2008-12-04 at the
       Wayback Machine
     * Association for Logic Programming (ALP)
     * Theory and Practice of Logic Programming (journal)
     * Logic programming in C++ with Castor
     * Logic programming Archived 2011-09-03 at the Wayback Machine in Oz
     * Prolog Development Center
     * Racklog: Logic Programming in Racket

     * v
     * t
     * e

   Programming paradigms (Comparison by language)

   Imperative

   Structured
     * Jackson structures
     * Block-structured
     * Modular
     * Non-structured
     * Procedural
     * Programming in the large and in the small
     * Design by contract
     * Invariant-based
     * Nested function

   Object-oriented
   (comparison, list)
     * Class-based, Prototype-based, Object-based
     * Agent
     * Immutable object
     * Persistent
     * Uniform Function Call Syntax

   Declarative

   Functional
   (comparison)
     * Recursive
     * Anonymous function (Partial application)
     * Higher-order
     * Purely functional
     * Total
     * Strict
     * GADTs
     * Dependent types
     * Functional logic
     * Point-free style
     * Expression-oriented
     * Applicative, Concatenative
     * Function-level, Value-level

   Dataflow
     * Flow-based
     * Reactive (Functional reactive)
     * Signals
     * Streams
     * Synchronous

   Logic
     * Abductive logic
     * Answer set
     * Constraint (Constraint logic)
     * Inductive logic
     * Nondeterministic
     * Ontology
     * Probabilistic logic
     * Query

   DSL
     * Algebraic modeling
     * Array
     * Automata-based (Action)
     * Command (Spacecraft)
     * Differentiable
     * End-user
     * Grammar-oriented
     * Interface description
     * Language-oriented
     * List comprehension
     * Low-code
     * Modeling
     * Natural language
     * Non-English-based
     * Page description
     * Pipes and filters
     * Probabilistic
     * Quantum
     * Scientific
     * Scripting
     * Set-theoretic
     * Simulation
     * Stack-based
     * System
     * Tactile
     * Templating
     * Transformation (Graph rewriting, Production, Pattern)
     * Visual

   Concurrent,
   distributed,
   parallel

     * Actor-based
     * Automatic mutual exclusion
     * Choreographic programming
     * Concurrent logic (Concurrent constraint logic)
     * Concurrent OO
     * Macroprogramming
     * Multitier programming
     * Organic computing
     * Parallel programming models
     * Partitioned global address space
     * Process-oriented
     * Relativistic programming
     * Service-oriented
     * Structured concurrency

   Metaprogramming

     * Attribute-oriented
     * Automatic (Inductive)
     * Dynamic
     * Extensible
     * Generic
     * Homoiconicity
     * Interactive
     * Macro (Hygienic)
     * Metalinguistic abstraction
     * Multi-stage
     * Program synthesis (Bayesian, Inferential, by demonstration, by
       example)
     * Reflective
     * Self-modifying code
     * Symbolic
     * Template

   Separation
   of concerns

     * Aspects
     * Components
     * Data-driven
     * Data-oriented
     * Event-driven
     * Features
     * Intentional
     * Literate
     * Roles
     * Subjects

     * v
     * t
     * e

   Types of programming languages

   Level

     * Machine
     * Assembly
     * Compiled
     * Interpreted

     * Low-level
     * High-level
     * Very high-level
     * Esoteric

   Generation

     * First
     * Second
     * Third
     * Fourth
     * Fifth

   Authority control databases: National Edit this at Wikidata
     * Germany
     * United States
     * France
     * BnF data
     * Czech Republic
     * Spain
     * Israel

   Retrieved from
   "https://en.wikipedia.org/w/index.php?title=Logic_programming&oldid=125
   9279203"

   Categories:
     * Logic programming
     * Computer-related introductions in 1972
     * Programming paradigms
     * Logic

   Hidden categories:
     * CS1 maint: DOI inactive as of November 2024
     * Articles with short description
     * Short description matches Wikidata
     * CS1: long volume value
     * Commons category link from Wikidata
     * Webarchive template wayback links

     * This page was last edited on 24 November 2024, at 08:48 (UTC).
     * Text is available under the Creative Commons Attribution-ShareAlike
       4.0 License; additional terms may apply. By using this site, you
       agree to the Terms of Use and Privacy Policy. Wikipedia® is a
       registered trademark of the Wikimedia Foundation, Inc., a
       non-profit organization.

     * Privacy policy
     * About Wikipedia
     * Disclaimers
     * Contact Wikipedia
     * Code of Conduct
     * Developers
     * Statistics
     * Cookie statement
     * Mobile view

     * Wikimedia Foundation
     * Powered by MediaWiki
